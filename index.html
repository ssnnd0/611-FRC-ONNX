Put your HTML text here<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Raspberry Pi Machine Learning Vision for FRC</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            background-color: black;
            color: white;
            cursor: none; /* Hide the default cursor */
            z-index: 999;
        }
        .custom-cursor {
            position: absolute;
            width: 10px;
            height: 10px;
            background-color: white;
            border-radius: 50%;
            pointer-events: none;
            z-index: 1000;
        }
        .logo {
            display: block;
            margin: 0 auto;
            width: 150px;
        }
        footer {
            text-align: center;
            margin-top: 20px;
            font-size: 0.8em;
        }
        canvas {
            display: block;
            position: absolute;
            top: 0;
            left: 0;
            height:100%;
            width:100%;
        }
    </style>
</head>
<body>
    <div class="custom-cursor" id="customCursor"></div>
    <canvas id="backgroundCanvas"></canvas>
    <img src="team611.png" alt="Team 611 FRC Logo" class="logo">
    <h1>Raspberry Pi Machine Learning Vision for FRC</h1>
    <h2>Inspiration</h2>
    <p>Vision tracking for FRC (FIRST Robotics Competition).</p>
    <h2>What does it do?</h2>
    <p>CustomVision.AI exports to Tensorflow and Onnx, which is used within the program to detect the object itself, then the Python program runs vision math on the supplied co-ordinates of any detected object to find out where it is in space. The program detects the object, and runs vision calculations to find distance to the object. It sends this information over the robot network using PyNetworkTables for additional processing on the roboRIO (e.g; pathfinding).</p>
    <h2>Next Steps</h2>
    <p>We can optimize our machine learning model by ditching CustomVision.AI and Tensorflow entirely and instead running the model on an OpenCV Dynamic Neural Network, and using all four cores on the Raspberry Pi. With this change, the vision program will be able to run in complete realtimeâ€”opening its use for on-the-fly pathfinding in the autonomous period and lining up with targets during the teleoperated period. Additionally, weâ€™d love to extract 3D-world co-ordinates from the detected object using OpenCV, allowing us to go from driving straight with PID loops to generating paths on the fly with Pathfinder to get our robot to swerve directly to its target.</p>
    <h3>Want to learn more?</h3>
    <ul>
        <li><a href="https://docs.google.com/document/d/1xEkql4t2k2on5pWODVsJKmNB83CbAXsfhYoOYy8iIx8/edit?usp=sharing">Google Doc using Tensorflow</a></li>
        <li><a href="https://docs.google.com/document/d/1wLhM5ahvdox7a_Fom5_leUtu3d5cdZXE6BZQBcUypsc/edit?usp=sharing">Google Doc using ONNX</a></li>
    </ul>
    <h2>ðŸ”’ Code License</h2>
    <p>This repository is licensed through the <code>The Unlicense</code>. More details are listed <a href="https://github.com/ssnnd0/611-FRC-VISIOn/blob/main/LICENSE">here</a>.</p>
    <pre>
        <code>
            HEHEHEHEHHE
        </code>
    </pre>
    <h3>Documentation</h3>
    <ul>
        <li><a href="https://onnx.ai/">ONNX Documentation</a></li>
        <li><a href="https://www.tensorflow.org/">TensorFlow Documentation</a></li>
    </ul>
    <footer>
        Â© Sandro Thornton / https://github.com/ssnnd0
    </footer>
    <script>
        const canvas = document.getElementById('backgroundCanvas');
        const ctx = canvas.getContext('2d');
        const dots = [];
        const numDots = 100;
        const maxDistance = 150;

        canvas.width = window.innerWidth;
        canvas.height = window.innerHeight;

        for (let i = 0; i < numDots; i++) {
            dots.push({
                x: Math.random() * canvas.width,
                y: Math.random() * canvas.height,
                vx: (Math.random() - 0.5) * 2,
                vy: (Math.random() - 0.5) * 2
            });
        }

        function draw() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            dots.forEach(dot => {
                dot.x += dot.vx;
                dot.y += dot.vy;

                if (dot.x < 0 || dot.x > canvas.width) dot.vx *= -1;
                if (dot.y < 0 || dot.y > canvas.height) dot.vy *= -1;

                ctx.beginPath();
                ctx.arc(dot.x, dot.y, 2, 0, Math.PI * 2);
                ctx.fillStyle = 'white';
                ctx.fill();
            });

            for (let i = 0; i < dots.length; i++) {
                for (let j = i + 1; j < dots.length; j++) {
                    const dx = dots[i].x - dots[j].x;
                    const dy = dots[i].y - dots[j].y;
                    const distance = Math.sqrt(dx * dx + dy * dy);

                    if (distance < maxDistance) {
                        ctx.beginPath();
                        ctx.moveTo(dots[i].x, dots[i].y);
                        ctx.lineTo(dots[j].x, dots[j].y);
                        ctx.strokeStyle = 'rgba(0, 255, 0, ' + (1 - distance / maxDistance) + ')';
                        ctx.stroke();
                    }
                }
            }

            requestAnimationFrame(draw);
        }

        document.addEventListener('mousemove', function(e) {
            const cursor = document.getElementById('customCursor');
            cursor.style.left = e.pageX + 'px';
            cursor.style.top = e.pageY + 'px';

            dots.push({
                x: e.pageX,
                y: e.pageY,
                vx: 0,
                vy: 0
            });

            if (dots.length > numDots) {
                dots.shift();
            }
        });

        draw();
    </script>
</body>
</html>
